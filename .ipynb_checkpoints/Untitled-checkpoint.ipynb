{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5131fb20",
   "metadata": {},
   "source": [
    "First thing I did was to import libraries and modules for building and training a Convolutional Neural Network (CNN) model. \n",
    "The libraries include:\n",
    "1. Matplotlib and Seaborn for Visualization.\n",
    "2. Keras Modules for Building the Model.\n",
    "3. ImageDataGenerator for Data Augmentation.\n",
    "4. Train-Test and Evaluation Metrics.\n",
    "5. Pandas for Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf91ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feded2d",
   "metadata": {},
   "source": [
    "This project is a machine learning project that uses the Sign Language MNIST dataset.\n",
    "The following code performs some preprocessing steps to prepare the data for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads the data from both the train and test sign mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63aaafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecd42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a333df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Labels (target values) for the training data from both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d744168",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc9774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing labels from both the train and test dataset from features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04349e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df['label']\n",
    "del test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce60f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b445330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer() # Initializes a LabelBinarizer object, which is used to convert the categorical labels into binary format.\n",
    "y_train = label_binarizer.fit_transform(y_train) #Transforms the training labels into binary format. Each label is converted into a binary vector indicating the presence of that label (one-hot encoding).\n",
    "y_test = label_binarizer.fit_transform(y_test) #Transforms the test labels in the same way as the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5878a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02d34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.values #Extracts the feature values from the training DataFrame and stores them in the x_train array.\n",
    "x_test = test_df.values #Extracts the feature values from the test DataFrame and stores them in the x_test array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9352bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255 #Scales the training features to the range [0, 1] by dividing each pixel value by 255.\n",
    "x_test = x_test / 255 #Scales the test features in the same way as the training features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5060261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52daf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,28,28,1) #Reshapes the training data to have a shape of (batch_size, height, width, channels), where each image is 28x28 pixels with 1 channel (grayscale).\n",
    "x_test = x_test.reshape(-1,28,28,1) #Reshapes the test data in the same way as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1c7b2",
   "metadata": {},
   "source": [
    "After the above preprocessing steps, the data is ready for use in training and evaluating a machine learning model (CNN) for\n",
    "recognizing sing language gestures based on the provided images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97bc78",
   "metadata": {},
   "source": [
    "We are going to initialize an 'ImageDataGenerator' object named 'datagen' with various augmentation options. The object wil be used to generate augmented training data by applying different transformations to the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532d013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, #No centering of input features around zero.\n",
    "        samplewise_center=False, #No centering of each sample around zero.\n",
    "        featurewise_std_normalization=False, #No feature-wise standard normalization.\n",
    "        samplewise_std_normalization=False, #No sample-wise standard normalization.\n",
    "        zca_whitening=False, #ZCA whitening is not applied. ZCA whitening is a preprocessing step that reduces redundancy in the input data.\n",
    "        rotation_range=10, #Randomly rotates images by up to 10 degrees.\n",
    "        zoom_range = 0.1, #Randomly zooms in or out by up to 10%.\n",
    "        width_shift_range=0.1, #Randomly shifts images horizontally by up to 10% of the image width.\n",
    "        height_shift_range=0.1, #Randomly shifts images vertically by up to 10% of the image height.\n",
    "        horizontal_flip=False, #No horizontal flipping.\n",
    "        vertical_flip=False) #No vertical flipping.\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28f3e6",
   "metadata": {},
   "source": [
    "After specifying these augmentation options, you use datagen.fit(x_train) to compute statistics (e.g., mean and standard deviation) on the training data. These statistics will be used to perform data augmentation during the training process. It's important to note that these transformations are applied only during training and not during evaluation.\n",
    "\n",
    "With the ImageDataGenerator set up, you're ready to proceed with creating a CNN model, compiling it, and then training it using the augmented data generated by datagen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed116686",
   "metadata": {},
   "source": [
    "After processing the images, the CNN model must be compiled to recognize all of the classes of information being used in the data, namely the 24 different groups of images. Normalization of the data must also be added to the data, equally balancing the classes with less images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7789afc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f4f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7101977",
   "metadata": {},
   "source": [
    "We have created a Convolutional Neural Network (CNN) model using the Keras Sequential API. \n",
    "This model consists of multiple layers, including convolutional, pooling, batch normalization, dropout, and dense layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728ec81",
   "metadata": {},
   "source": [
    "We have now defined the architecture of your CNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b54ed1",
   "metadata": {},
   "source": [
    "The next steps involve compiling the model, specifying the optimizer and loss function, and training the model using the augmented data generated by ImageDataGenerator. \n",
    "Once trained, you can evaluate the model's performance on the test dataset and make predictions on new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d82c3e",
   "metadata": {},
   "source": [
    "Finally, defining the loss functions and metrics along with fitting the model to the data will create our Sign Language Recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f2ffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 28, 75)        300       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 75)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 14, 14, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 50)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 7, 7, 25)          100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 25)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264049 (1.01 MB)\n",
      "Trainable params: 263749 (1.01 MB)\n",
      "Non-trainable params: 300 (1.17 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "215/215 [==============================] - 40s 177ms/step - loss: 1.0982 - accuracy: 0.6558 - val_loss: 4.1208 - val_accuracy: 0.0746\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 40s 187ms/step - loss: 0.2257 - accuracy: 0.9258 - val_loss: 1.0960 - val_accuracy: 0.6161\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 0.1082 - accuracy: 0.9656 - val_loss: 0.1790 - val_accuracy: 0.9416\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 40s 184ms/step - loss: 0.0596 - accuracy: 0.9811 - val_loss: 0.0699 - val_accuracy: 0.9770\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 41s 191ms/step - loss: 0.0496 - accuracy: 0.9842 - val_loss: 1.1274 - val_accuracy: 0.7139\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 40s 188ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.0260 - val_accuracy: 0.9923\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 40s 187ms/step - loss: 0.0297 - accuracy: 0.9905 - val_loss: 0.0198 - val_accuracy: 0.9941\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 40s 187ms/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.2364 - val_accuracy: 0.9318\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.3194 - val_accuracy: 0.9045\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 43s 201ms/step - loss: 0.0232 - accuracy: 0.9933 - val_loss: 0.0088 - val_accuracy: 0.9972\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 0.0249 - accuracy: 0.9919 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.0262 - accuracy: 0.9913 - val_loss: 0.1248 - val_accuracy: 0.9600\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 43s 202ms/step - loss: 0.0178 - accuracy: 0.9938 - val_loss: 0.0296 - val_accuracy: 0.9904\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.0172 - accuracy: 0.9943 - val_loss: 0.0212 - val_accuracy: 0.9916\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 45s 208ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.0298 - val_accuracy: 0.9880\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 45s 208ms/step - loss: 0.0162 - accuracy: 0.9941 - val_loss: 0.0102 - val_accuracy: 0.9971\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 44s 206ms/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.0070 - val_accuracy: 0.9978\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 44s 205ms/step - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.0858 - val_accuracy: 0.9718\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 45s 208ms/step - loss: 0.0219 - accuracy: 0.9924 - val_loss: 0.1139 - val_accuracy: 0.9605\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0174 - val_accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SammyNgure\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy']) #Compiles the model with the Adam optimizer, categorical cross-entropy loss (appropriate for multi-class classification), and the accuracy metric.\n",
    "model.summary() #Displays a summary of the model architecture, including the number of parameters and layer configurations.\n",
    "\n",
    "#Trains the model using the augmented training data generated by the ImageDataGenerator. The training is performed over 20 epochs and includes validation on the test dataset.\n",
    "history = model.fit(datagen.flow(x_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (x_test, y_test))\n",
    "\n",
    "#Saves the trained model to a file named \"smnist.h5\". This file contains the model architecture, weights, and optimizer configuration.\n",
    "model.save('smnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da6319",
   "metadata": {},
   "source": [
    "Now, using two popular live video processing libraries known as Mediapipe and Open-CV, we can take webcam input and run our previously developed model on real time video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158c5bb",
   "metadata": {},
   "source": [
    "To start, we need to import the required packages for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3002de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd4a22",
   "metadata": {},
   "source": [
    "We then need to set up a real-time hand gesture recognition application using TensorFlow, OpenCV, and MediaPipe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680bb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Trained Model\n",
    "model = load_model('smnist.h5') \n",
    "\n",
    "#MediaPipe Hand Detection.\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#Opening a Webcam Stream.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Frame Dimensions\n",
    "_, frame = cap.read()\n",
    "h, w, c = frame.shape\n",
    "\n",
    "#Variables for Analysis\n",
    "analysisframe = ''\n",
    "\n",
    "#List of Gesture Labels.\n",
    "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926e432",
   "metadata": {},
   "source": [
    "We have set up the initial components necessary for real-time hand gesture recognition. To complete the application, you'll need to process the frames from the webcam, detect hands using MediaPipe, preprocess the detected hands for input to your trained model, make predictions, and display the results on the frame. Remember to loop through the frames, process each frame, and update the display in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152493f",
   "metadata": {},
   "source": [
    "The following code effectively captures frames, detects hands, highlights them with bounding boxes, and draws landmarks and connections on the hands. It provides real-time visualizations of hand gesstures using your trained model. You can further extend this code to process the hand regions, preprocess them, and make predictions using the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2589e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m framergb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframergb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m hand_landmarks \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hand_landmarks:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            y_min -= 20\n",
    "            y_max += 20\n",
    "            x_min -= 20\n",
    "            x_max += 20\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dbac0",
   "metadata": {},
   "source": [
    "The second to last part of the program is capturing a single frame on cue, cropping it to the dimensions of the bouding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6083551",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        # SPACE pressed\n",
    "        analysisframe = frame\n",
    "        showframe = analysisframe\n",
    "        cv2.imshow(\"Frame\", showframe)\n",
    "        framergbanalysis = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2RGB)\n",
    "        resultanalysis = hands.process(framergbanalysis)\n",
    "        hand_landmarksanalysis = resultanalysis.multi_hand_landmarks\n",
    "        if hand_landmarksanalysis:\n",
    "            for handLMsanalysis in hand_landmarksanalysis:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lmanalysis in handLMsanalysis.landmark:\n",
    "                    x, y = int(lmanalysis.x * w), int(lmanalysis.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20 \n",
    "\n",
    "        analysisframe = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2GRAY)\n",
    "        analysisframe = analysisframe[y_min:y_max, x_min:x_max]\n",
    "        analysisframe = cv2.resize(analysisframe,(28,28))\n",
    "\n",
    "\n",
    "        nlist = []\n",
    "        rows,cols = analysisframe.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                k = analysisframe[i,j]\n",
    "                nlist.append(k)\n",
    "        \n",
    "        datan = pd.DataFrame(nlist).T\n",
    "        colname = []\n",
    "        for val in range(784):\n",
    "            colname.append(val)\n",
    "        datan.columns = colname\n",
    "\n",
    "        pixeldata = datan.values\n",
    "        pixeldata = pixeldata / 255\n",
    "        pixeldata = pixeldata.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c737910",
   "metadata": {},
   "source": [
    "Finally, we need to run the trained model on the processed image and process the information output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b57b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using the model\n",
    "prediction = model.predict(pixeldata)\n",
    "predarray = np.array(prediction[0])\n",
    "\n",
    "#Dictionary of Letter Predictions\n",
    "letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
    "\n",
    "#Sorting Prediction Array\n",
    "predarrayordered = sorted(predarray, reverse=True)\n",
    "high1 = predarrayordered[0]\n",
    "high2 = predarrayordered[1]\n",
    "high3 = predarrayordered[2]\n",
    "for key,value in letter_prediction_dict.items():\n",
    "    if value==high1:\n",
    "        print(\"Predicted Character 1: \", key)\n",
    "        print('Confidence 1: ', 100*value)\n",
    "    elif value==high2:\n",
    "        print(\"Predicted Character 2: \", key)\n",
    "        print('Confidence 2: ', 100*value)\n",
    "    elif value==high3:\n",
    "        print(\"Predicted Character 3: \", key)\n",
    "        print('Confidence 3: ', 100*value)\n",
    "        \n",
    "#Delay Before Next Frame\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2491a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
